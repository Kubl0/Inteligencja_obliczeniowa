{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Zagadnieniem tej pracy domowej było wybranie bazy danych oraz zbudowanie modeli klasyfikacji do niej, tak aby sprawdzić ich działanie. Moja baza danych opisywała wiele różnych\n",
    "parametrów wina, takich jak kwasowość, cukry, siarczyny oraz alkohol i tym podobne oraz jego ocena. Zadaniem było stworzenie modelu, który na podstawie tych parametrów będzie\n",
    "w stanie określić, jaką ocena wino otrzymało. W tym celu wykorzystałem 4 różne modele: drzewo decyzyjne, k-Najbliższych sąsiadów, Naive Bayes oraz sieć neuronową. Wszystkie\n",
    "modele zostały przetestowane na danych treningowych, a następnie na danych testowych. Przetestowane zostało wiele parametrów danych modeli."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Funkcje odpowiadające za wczytanie danych i ich preprocessing:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wczytane i pomieszane dane: \n",
      "       type  fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
      "5253    red            6.7              0.75         0.01            2.40   \n",
      "1688  white            6.7              0.25         0.26            1.55   \n",
      "6259    red            8.3              0.85         0.14            2.50   \n",
      "4844  white            4.8              0.29         0.23            1.10   \n",
      "3417  white            6.7              0.64         0.30            1.20   \n",
      "\n",
      "      chlorides  free sulfur dioxide  total sulfur dioxide  density    pH  \\\n",
      "5253      0.078                 17.0                  32.0  0.99550  3.55   \n",
      "1688      0.041                118.5                 216.0  0.99490  3.55   \n",
      "6259      0.093                 13.0                  54.0  0.99724  3.36   \n",
      "4844      0.044                 38.0                 180.0  0.98924  3.28   \n",
      "3417      0.030                 18.0                  76.0  0.98920  3.16   \n",
      "\n",
      "      sulphates  alcohol quality  \n",
      "5253       0.61     12.8  medium  \n",
      "1688       0.63      9.4     low  \n",
      "6259       0.54     10.1     low  \n",
      "4844       0.34     11.9  medium  \n",
      "3417       0.60     12.9     low  \n",
      "Dane po normalizacji: \n",
      "      type  fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
      "2790   1.0       0.272727          0.080000     0.186747        0.095859   \n",
      "2959   1.0       0.289256          0.213333     0.156627        0.074387   \n",
      "2903   1.0       0.157025          0.086667     0.150602        0.007669   \n",
      "5252   0.0       0.190083          0.086667     0.240964        0.012270   \n",
      "497    1.0       0.247934          0.100000     0.192771        0.015337   \n",
      "\n",
      "      chlorides  free sulfur dioxide  total sulfur dioxide   density  \\\n",
      "2790   0.073090             0.107639              0.472350  0.168884   \n",
      "2959   0.011628             0.086806              0.193548  0.046270   \n",
      "2903   0.043189             0.086806              0.172811  0.059572   \n",
      "5252   0.094684             0.137153              0.366359  0.078851   \n",
      "497    0.028239             0.145833              0.324885  0.063428   \n",
      "\n",
      "            pH  sulphates   alcohol quality  \n",
      "2790  0.457364   0.207865  0.347826  medium  \n",
      "2959  0.093023   0.179775  0.753623  medium  \n",
      "2903  0.457364   0.168539  0.492754  medium  \n",
      "5252  0.410853   0.207865  0.565217  medium  \n",
      "497   0.441860   0.179775  0.652174  medium  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def getData(directory):\n",
    "    data = pd.read_csv(directory)\n",
    "    data = shuffle(data)\n",
    "    data['quality'] = data['quality'].replace([3, 4, 5], 'low')\n",
    "    data['quality'] = data['quality'].replace([6, 7], 'medium')\n",
    "    data['quality'] = data['quality'].replace([8, 9], 'high')\n",
    "    return data\n",
    "\n",
    "def withDeletion(df):\n",
    "    label_encoder = LabelEncoder()\n",
    "    df[\"type\"] = label_encoder.fit_transform(df[\"type\"])\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "def withMean(df):\n",
    "    label_encoder = LabelEncoder()\n",
    "    df[\"type\"] = label_encoder.fit_transform(df[\"type\"])\n",
    "    df = df.fillna(df.mean(numeric_only=True))\n",
    "    return df\n",
    "\n",
    "def normalization(df):\n",
    "    scaler = MinMaxScaler()\n",
    "    columns_to_normalize = df.columns[:-1]\n",
    "    df[columns_to_normalize] = scaler.fit_transform(df[columns_to_normalize])\n",
    "    return df\n",
    "\n",
    "print(\"Wczytane i pomieszane dane: \")\n",
    "print(getData(\"./Data/winequality.csv\").head())\n",
    "print(\"Dane po normalizacji: \")\n",
    "print(normalization(withMean(getData(\"./Data/winequality.csv\"))).head())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-07T17:32:14.159804400Z",
     "start_time": "2023-05-07T17:32:14.068802100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Są tu dwie funkcje odpowiadające za preprocessing danych. Pierwsza z nich usuwa wszystkie wiersze, które zawierają jakąś wartość NaN. Druga z nich zastępuje wartości NaN średnią."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pierwszym z modeli jest Drzewo Decyzyjne, ma ono 2 wersje, 1 gdzie maksymalny poziom wynosi 9, a druga gdzie maksymalny poziom drzewa wynosi 5. Kod wygląda następująco:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def classify(df, train_size):\n",
    "    train_set, test_set = train_test_split(df, train_size=train_size)\n",
    "    classifier = DecisionTreeClassifier(criterion=\"entropy\", max_depth=9, min_samples_leaf=2, splitter=\"best\",\n",
    "                                        class_weight=None, random_state=101)\n",
    "    classifier.fit(train_set.iloc[:, :-1], train_set.iloc[:, -1])\n",
    "    accurarcy = classifier.score(test_set.iloc[:, :-1], test_set.iloc[:, -1])\n",
    "    predicted = classifier.predict(test_set.iloc[:, :-1])\n",
    "    confusionMatrix = confusion_matrix(test_set.iloc[:, -1], predicted)\n",
    "    return accurarcy, confusionMatrix\n",
    "\n",
    "\n",
    "def classifyMax(df, train_size, max_depth):\n",
    "    train_set, test_set = train_test_split(df, train_size=train_size, random_state=2137)\n",
    "    classifier = DecisionTreeClassifier(max_depth=max_depth)\n",
    "    classifier.fit(train_set.iloc[:, :-1], train_set.iloc[:, -1])\n",
    "    accurarcy = classifier.score(test_set.iloc[:, :-1], test_set.iloc[:, -1])\n",
    "    predicted = classifier.predict(test_set.iloc[:, :-1])\n",
    "    confusionMatrix = confusion_matrix(test_set.iloc[:, -1], predicted)\n",
    "    return accurarcy, confusionMatrix"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-07T17:32:14.203301400Z",
     "start_time": "2023-05-07T17:32:14.160805500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without max-depth: \n",
      "Without normalization:\n",
      "DT w/ Deletion: \n",
      "Accurarcy:  0.714064914992272\n",
      "DT w/ Mean: \n",
      "Accurarcy:  0.6984615384615385\n",
      "With normalization:\n",
      "DT w/ Deletion: \n",
      "Accurarcy:  0.7094281298299846\n",
      "DT w/ Mean: \n",
      "Accurarcy:  0.7415384615384616\n",
      "With max-depth: \n",
      "Without normalization:\n",
      "DT w/ Deletion: \n",
      "Accurarcy:  0.6877897990726429\n",
      "DT w/ Mean: \n",
      "Accurarcy:  0.6938461538461539\n",
      "With normalization:\n",
      "DT w/ Deletion: \n",
      "Accurarcy:  0.6877897990726429\n",
      "DT w/ Mean: \n",
      "Accurarcy:  0.6938461538461539\n"
     ]
    }
   ],
   "source": [
    "from Methods import DecisionTree\n",
    "from Methods import Preprocessing\n",
    "\n",
    "TRAIN_SIZE = 0.9\n",
    "\n",
    "\n",
    "def main():\n",
    "    dataD = Preprocessing.withDeletion(getData(\"./Data/winequality.csv\"))\n",
    "    dataM = Preprocessing.withMean(getData(\"./Data/winequality.csv\"))\n",
    "\n",
    "    getResultsOfDT(dataD, dataM)\n",
    "\n",
    "\n",
    "\n",
    "def printResult(result):\n",
    "    print(\"Accurarcy: \", result[0])\n",
    "    # print(\"Confusion Matrix: \")\n",
    "    # print(result[1])\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def getResultsOfDT(dataD, dataM):\n",
    "    print(\"Without max-depth: \")\n",
    "    print(\"Without normalization:\")\n",
    "    print(\"DT w/ Deletion: \")\n",
    "    print(printResult(DecisionTree.classify(dataD, TRAIN_SIZE)), end='')\n",
    "    print(\"DT w/ Mean: \")\n",
    "    print(printResult(DecisionTree.classify(dataM, TRAIN_SIZE)), end='')\n",
    "    print(\"With normalization:\")\n",
    "    print(\"DT w/ Deletion: \")\n",
    "    print(printResult(DecisionTree.classify(Preprocessing.normalization(dataD), TRAIN_SIZE)), end='')\n",
    "    print(\"DT w/ Mean: \")\n",
    "    print(printResult(DecisionTree.classify(Preprocessing.normalization(dataM), TRAIN_SIZE)), end='')\n",
    "    print(\"With max-depth: \")\n",
    "    print(\"Without normalization:\")\n",
    "    print(\"DT w/ Deletion: \")\n",
    "    print(printResult(DecisionTree.classifyMax(dataD, TRAIN_SIZE, 5)), end='')\n",
    "    print(\"DT w/ Mean: \")\n",
    "    print(printResult(DecisionTree.classifyMax(dataM, TRAIN_SIZE, 5)), end='')\n",
    "    print(\"With normalization:\")\n",
    "    print(\"DT w/ Deletion: \")\n",
    "    print(printResult(DecisionTree.classifyMax(Preprocessing.normalization(dataD), TRAIN_SIZE, 5)), end='')\n",
    "    print(\"DT w/ Mean: \")\n",
    "    print(printResult(DecisionTree.classifyMax(Preprocessing.normalization(dataM), TRAIN_SIZE, 5)), end='')\n",
    "\n",
    "main()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-07T17:32:14.562302Z",
     "start_time": "2023-05-07T17:32:14.177805400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Wyniki są następujące, dla drzewa w wersji z 9 poziomami wyniki są lepsze niż dla tego z 5 poziomami. Przy 5 poziomach normalizacja nie miała znaczenia, gdzie w przypadku 9 poziomów\n",
    "normalizacja pogorszyła wyniki. Ustalenie 9 poziomów wynikało z przetestowania metod z wieloma roznymi parametrami przy użyciu GridSearchCV. W tym wypadku usunięcie danych było lepszym wyjściem."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Kolejnym modelem jest Naiwny Bayes, dla którego kod wygląda następująco:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from Methods import NaiveBayes_KN as NBK\n",
    "\n",
    "def classifyNB(df, train_size):\n",
    "    train_set, test_set = train_test_split(df, train_size=train_size, random_state=2137)\n",
    "\n",
    "    classifier = GaussianNB()\n",
    "    classifier.fit(train_set.iloc[:, :-1], train_set.iloc[:, -1])\n",
    "    accurarcy = classifier.score(test_set.iloc[:, :-1], test_set.iloc[:, -1])\n",
    "    predicted = classifier.predict(test_set.iloc[:, :-1])\n",
    "    confusionMatrix = confusion_matrix(test_set.iloc[:, -1], predicted)\n",
    "    return accurarcy, confusionMatrix"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-07T17:32:14.577805200Z",
     "start_time": "2023-05-07T17:32:14.564305Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without normalization:\n",
      "NB w/ Deletion: \n",
      "Accurarcy:  0.6259659969088099\n",
      "NB w/ Mean: \n",
      "Accurarcy:  0.6307692307692307\n",
      "With normalization:\n",
      "NB w/ Deletion: \n",
      "Accurarcy:  0.6136012364760433\n",
      "NB w/ Mean: \n",
      "Accurarcy:  0.6230769230769231\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    dataD = Preprocessing.withDeletion(getData(\"./Data/winequality.csv\"))\n",
    "    dataM = Preprocessing.withMean(getData(\"./Data/winequality.csv\"))\n",
    "\n",
    "    getResultsOfNB(dataD, dataM)\n",
    "\n",
    "def getResultsOfNB(dataD, dataM):\n",
    "    print(\"Without normalization:\")\n",
    "    print(\"NB w/ Deletion: \")\n",
    "    print(printResult(NBK.classifyNB(dataD, TRAIN_SIZE)), end='')\n",
    "    print(\"NB w/ Mean: \")\n",
    "    print(printResult(NBK.classifyNB(dataM, TRAIN_SIZE)), end='')\n",
    "    print(\"With normalization:\")\n",
    "    print(\"NB w/ Deletion: \")\n",
    "    print(printResult(NBK.classifyNB(Preprocessing.normalization(dataD), TRAIN_SIZE)), end='')\n",
    "    print(\"NB w/ Mean: \")\n",
    "    print(printResult(NBK.classifyNB(Preprocessing.normalization(dataM), TRAIN_SIZE)), end='')\n",
    "\n",
    "main()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-07T17:32:14.729801300Z",
     "start_time": "2023-05-07T17:32:14.582803700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Wyniki dla normalizacji jak i bez niej są bardzo podobne. Minimalnie na korzyść tych bez normalizacji. Lepsze wyniki wychodzą dla danych z uzupełnionymi brakami."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Kolejnym modelem jest kNN, dla którego kod wygląda następująco:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def classifyKN(df, train_size, neighbors):\n",
    "    train_set, test_set = train_test_split(df, train_size=train_size, random_state=2137)\n",
    "\n",
    "    classifier = KNeighborsClassifier(n_neighbors=neighbors)\n",
    "    classifier.fit(train_set.iloc[:, :-1], train_set.iloc[:, -1])\n",
    "    accurarcy = classifier.score(test_set.iloc[:, :-1], test_set.iloc[:, -1])\n",
    "    predicted = classifier.predict(test_set.iloc[:, :-1])\n",
    "    confusionMatrix = confusion_matrix(test_set.iloc[:, -1], predicted)\n",
    "    return accurarcy, confusionMatrix"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-07T17:32:14.730801800Z",
     "start_time": "2023-05-07T17:32:14.709303600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without normalization:\n",
      "KNN w/ Deletion: \n",
      "Accurarcy:  0.6367851622874807\n",
      "KNN w/ Mean: \n",
      "Accurarcy:  0.6230769230769231\n",
      "KNN w/ Deletion: \n",
      "Accurarcy:  0.6290571870170015\n",
      "KNN w/ Mean: \n",
      "Accurarcy:  0.6215384615384615\n",
      "KNN w/ Deletion: \n",
      "Accurarcy:  0.6259659969088099\n",
      "KNN w/ Mean: \n",
      "Accurarcy:  0.6246153846153846\n",
      "With normalization:\n",
      "KNN w/ Deletion: \n",
      "Accurarcy:  0.7279752704791345\n",
      "KNN w/ Mean: \n",
      "Accurarcy:  0.7184615384615385\n",
      "KNN w/ Deletion: \n",
      "Accurarcy:  0.7279752704791345\n",
      "KNN w/ Mean: \n",
      "Accurarcy:  0.7076923076923077\n",
      "KNN w/ Deletion: \n",
      "Accurarcy:  0.7295208655332303\n",
      "KNN w/ Mean: \n",
      "Accurarcy:  0.7107692307692308\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    dataD = Preprocessing.withDeletion(getData(\"./Data/winequality.csv\"))\n",
    "    dataM = Preprocessing.withMean(getData(\"./Data/winequality.csv\"))\n",
    "    getResultsOfKNN(dataD, dataM)\n",
    "\n",
    "def getResultsOfKNN(dataD, dataM):\n",
    "    print(\"Without normalization:\")\n",
    "    for i in range(10, 13):\n",
    "        print(\"KNN w/ Deletion: \")\n",
    "        print(printResult(NBK.classifyKN(dataD, TRAIN_SIZE, i)), end='')\n",
    "        print(\"KNN w/ Mean: \")\n",
    "        print(printResult(NBK.classifyKN(dataM, TRAIN_SIZE, i)), end='')\n",
    "    print(\"With normalization:\")\n",
    "    for i in range(10, 13):\n",
    "        print(\"KNN w/ Deletion: \")\n",
    "        print(printResult(NBK.classifyKN(Preprocessing.normalization(dataD), TRAIN_SIZE, i)), end='')\n",
    "        print(\"KNN w/ Mean: \")\n",
    "        print(printResult(NBK.classifyKN(Preprocessing.normalization(dataM), TRAIN_SIZE, i)), end='')\n",
    "\n",
    "main()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-07T17:32:16.003801300Z",
     "start_time": "2023-05-07T17:32:14.720304400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Najlepsze wyniki są dla n=12, określiłem to przy użyciu tej pętli tutaj oraz również przy użyciu GridSearchCV. Przy użyciu normalizacji wyniki są zdecydowanie lepsze z nią. Wyniki są również\n",
    "lepsze dla uzupełnienia braków średnią niż usunięciem danych."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ostatnim modelem jest sieć neuronowa, dla której kod wygląda następująco:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "def classify(df, train_size):\n",
    "    train_set, test_set = train_test_split(df, train_size=train_size, random_state=2137)\n",
    "\n",
    "    # param_grid = {\n",
    "    #     'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50), (100, 50, 25), (10, 20, 30, 40), (100,50,100)],\n",
    "    #     'activation': ['relu', 'tanh'],\n",
    "    #     'solver': ['adam', 'sgd'],\n",
    "    # }\n",
    "    #\n",
    "    # classifier = MLPClassifier()\n",
    "    # grid_search = GridSearchCV(classifier, param_grid, cv=5)\n",
    "    # grid_search.fit(train_set.iloc[:, :-1], train_set.iloc[:, -1])\n",
    "    # print(grid_search.best_params_)\n",
    "    # print(grid_search.best_score_)\n",
    "\n",
    "    classifier = MLPClassifier(hidden_layer_sizes=(100, 50, 25), activation='tanh', solver='adam', max_iter=2000)\n",
    "    classifier.fit(train_set.iloc[:, :-1], train_set.iloc[:, -1])\n",
    "\n",
    "    predicted = classifier.predict(test_set.iloc[:, :-1])\n",
    "    confusionMatrix = confusion_matrix(test_set.iloc[:, -1], predicted)\n",
    "    accurarcy = accuracy_score(test_set.iloc[:, -1], predicted)\n",
    "    return accurarcy, confusionMatrix"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-07T17:32:16.020803Z",
     "start_time": "2023-05-07T17:32:16.004802400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Użyty znowu został GridSearchCV, aby dobrać najlepsze parametry. Wyniki dla sieci neuronowej są bardzo dobre, a najlepsze są dla 100, 50, 25 neuronów w kolejnych warstwach.\n",
    "Wyniki są następujące:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurarcy:  0.7418856259659969\n",
      "Neural w/ Mean: \n",
      "Accurarcy:  0.703076923076923\n",
      "With normalization:\n",
      "Neural w/ Deletion: \n",
      "Accurarcy:  0.7217928902627512\n",
      "Neural w/ Mean: \n",
      "Accurarcy:  0.7307692307692307\n",
      "Without normalization:\n",
      "Neural w/ Deletion: \n",
      "Accurarcy:  0.7279752704791345\n",
      "Neural w/ Mean: \n",
      "Accurarcy:  0.7169230769230769\n",
      "With normalization:\n",
      "Neural w/ Deletion: \n",
      "Accurarcy:  0.7465224111282844\n",
      "Neural w/ Mean: \n"
     ]
    }
   ],
   "source": [
    "from Methods import Neural\n",
    "\n",
    "def main():\n",
    "    dataD = Preprocessing.withDeletion(getData(\"./Data/winequality.csv\"))\n",
    "    dataM = Preprocessing.withMean(getData(\"./Data/winequality.csv\"))\n",
    "\n",
    "    getResultsOfNeural(dataD, dataM)\n",
    "\n",
    "\n",
    "LAYERS = (10, 7)\n",
    "\n",
    "\n",
    "def getResultsOfNeural(dataD, dataM):\n",
    "    print(\"Without normalization:\")\n",
    "    print(\"Neural w/ Deletion: \")\n",
    "    print(printResult(Neural.classify(dataD, TRAIN_SIZE, LAYERS)), end='')\n",
    "    print(\"Neural w/ Mean: \")\n",
    "    print(printResult(Neural.classify(dataM, TRAIN_SIZE, LAYERS)), end='')\n",
    "    print(\"With normalization:\")\n",
    "    print(\"Neural w/ Deletion: \")\n",
    "    print(printResult(Neural.classify(Preprocessing.normalization(dataD), TRAIN_SIZE, LAYERS)), end='')\n",
    "    print(\"Neural w/ Mean: \")\n",
    "    print(printResult(Neural.classify(Preprocessing.normalization(dataM), TRAIN_SIZE, LAYERS)), end='')\n",
    "\n",
    "main()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Wyniki są lepsze dla danych z normalizacją oraz dla uzupełnienia danych średnią."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Macierze błedów dla wszystkich modeli wyglądają następująco:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without max-depth: \n",
      "Without normalization:\n",
      "DT w/ Deletion: \n",
      "Confusion Matrix: \n",
      "[[  4   0  14]\n",
      " [  1 179  57]\n",
      " [ 11  89 292]]\n",
      "DT w/ Mean: \n",
      "Confusion Matrix: \n",
      "[[  7   0  18]\n",
      " [  3 162  64]\n",
      " [ 10  91 295]]\n",
      "With normalization:\n",
      "DT w/ Deletion: \n",
      "Confusion Matrix: \n",
      "[[  4   1  15]\n",
      " [  1 177  76]\n",
      " [  2  90 281]]\n",
      "DT w/ Mean: \n",
      "Confusion Matrix: \n",
      "[[  3   0   9]\n",
      " [  4 163  80]\n",
      " [  6  79 306]]\n",
      "With max-depth: \n",
      "Without normalization:\n",
      "DT w/ Deletion: \n",
      "Confusion Matrix: \n",
      "[[  0   0  22]\n",
      " [  0 137  99]\n",
      " [  0  77 312]]\n",
      "DT w/ Mean: \n",
      "Confusion Matrix: \n",
      "[[  0   0  16]\n",
      " [  0 152  95]\n",
      " [  0  85 302]]\n",
      "With normalization:\n",
      "DT w/ Deletion: \n",
      "Confusion Matrix: \n",
      "[[  0   0  22]\n",
      " [  0 137  99]\n",
      " [  0  77 312]]\n",
      "DT w/ Mean: \n",
      "Confusion Matrix: \n",
      "[[  0   0  16]\n",
      " [  0 152  95]\n",
      " [  0  85 302]]\n",
      "Without normalization:\n",
      "NB w/ Deletion: \n",
      "Confusion Matrix: \n",
      "[[  4   1  17]\n",
      " [  2 107 127]\n",
      " [ 32  87 270]]\n",
      "NB w/ Mean: \n",
      "Confusion Matrix: \n",
      "[[  2   2  12]\n",
      " [  1 130 116]\n",
      " [ 39  98 250]]\n",
      "With normalization:\n",
      "NB w/ Deletion: \n",
      "Confusion Matrix: \n",
      "[[  4   1  17]\n",
      " [  2 107 127]\n",
      " [ 32  87 270]]\n",
      "NB w/ Mean: \n",
      "Confusion Matrix: \n",
      "[[  2   2  12]\n",
      " [  1 130 116]\n",
      " [ 39  98 250]]\n",
      "Without normalization:\n",
      "KNN w/ Deletion: \n",
      "Confusion Matrix: \n",
      "[[  1   2  19]\n",
      " [  0 152  84]\n",
      " [  4  72 313]]\n",
      "KNN w/ Mean: \n",
      "Confusion Matrix: \n",
      "[[  2   0  14]\n",
      " [  0 156  91]\n",
      " [  5  61 321]]\n",
      "With normalization:\n",
      "KNN w/ Deletion: \n",
      "Confusion Matrix: \n",
      "[[  1   2  19]\n",
      " [  0 152  84]\n",
      " [  4  72 313]]\n",
      "KNN w/ Mean: \n",
      "Confusion Matrix: \n",
      "[[  2   0  14]\n",
      " [  0 156  91]\n",
      " [  5  61 321]]\n",
      "Without normalization:\n",
      "Neural w/ Deletion: \n",
      "Confusion Matrix: \n",
      "[[  5   0  17]\n",
      " [  0 157  79]\n",
      " [  0  63 326]]\n",
      "Neural w/ Mean: \n",
      "Confusion Matrix: \n",
      "[[  4   1  11]\n",
      " [  0 178  69]\n",
      " [ 12  55 320]]\n",
      "With normalization:\n",
      "Neural w/ Deletion: \n",
      "Confusion Matrix: \n",
      "[[  8   0  14]\n",
      " [  3 168  65]\n",
      " [  9  51 329]]\n",
      "Neural w/ Mean: \n",
      "Confusion Matrix: \n",
      "[[  7   1   8]\n",
      " [  0 170  77]\n",
      " [  3  62 322]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    dataD = Preprocessing.withDeletion(getData(\"./Data/winequality.csv\"))\n",
    "    dataM = Preprocessing.withMean(getData(\"./Data/winequality.csv\"))\n",
    "\n",
    "    getResultsOfDT(dataD, dataM)\n",
    "    getResultsOfNB(dataD, dataM)\n",
    "    getResultsOfKNN(dataD, dataM)\n",
    "    getResultsOfNeural(dataD, dataM)\n",
    "\n",
    "    return 0\n",
    "\n",
    "\n",
    "def getData(directory):\n",
    "    data = pd.read_csv(directory)\n",
    "    data = shuffle(data)\n",
    "    data['quality'] = data['quality'].replace([3, 4, 5], 'low')\n",
    "    data['quality'] = data['quality'].replace([6, 7], 'medium')\n",
    "    data['quality'] = data['quality'].replace([8, 9], 'high')\n",
    "    return data\n",
    "\n",
    "\n",
    "def printResult(result):\n",
    "    print(\"Confusion Matrix: \")\n",
    "    print(result[1])\n",
    "    return \"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-07T17:32:14.065808500Z",
     "start_time": "2023-05-07T17:28:00.296494700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Podsumowująć w większości przypadków normalizacja danych daje lepsze wyniki, a uzupełnienie braków danych średnią daje lepsze wyniki niż samo usunięcie danych. Najlepszym modelem okazała się sieć neuronowa oraz kNN, gdzie wyniki były na poziomie 75%. Najgorszym modelem okazał się naiwny klasyfikator bayesowski, gdzie wyniki były na poziomie 60%. Wszystkie modele testowane były na 90% danych treningowych oraz 10% danych testowych, bo dla takich wyniki były najlepsze."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
