{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Zagadnieniem tej pracy domowej było wybranie bazy danych oraz zbudowanie modeli klasyfikacji do niej, tak aby sprawdzić ich działanie. Moja baza danych opisywała wiele różnych\n",
    "parametrów wina, takich jak kwasowość, cukry, siarczyny oraz alkohol i tym podobne oraz jego ocena. Zadaniem było stworzenie modelu, który na podstawie tych parametrów będzie\n",
    "w stanie określić, jaką ocena wino otrzymało. W tym celu wykorzystałem 4 różne modele: drzewo decyzyjne, k-Najbliższych sąsiadów, Naive Bayes oraz sieć neuronową. Wszystkie\n",
    "modele zostały przetestowane na danych treningowych, a następnie na danych testowych. Przetestowane zostało wiele parametrów danych modeli."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Funkcje odpowiadające za wczytanie danych i ich preprocessing:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wczytane i pomieszane dane: \n",
      "       type  fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
      "3475  white            6.2              0.36         0.45            10.4   \n",
      "1137  white            6.1              0.31         0.37             8.4   \n",
      "4460  white            6.6              0.18         0.26            17.3   \n",
      "4508  white            5.8              0.26         0.30             2.6   \n",
      "4247  white            6.6              0.36         0.47             1.4   \n",
      "\n",
      "      chlorides  free sulfur dioxide  total sulfur dioxide  density    pH  \\\n",
      "3475      0.060                 22.0                 184.0  0.99711  3.31   \n",
      "1137      0.031                 70.0                 170.0  0.99340  3.42   \n",
      "4460      0.051                 17.0                 149.0  0.99840  3.00   \n",
      "4508      0.034                 75.0                 129.0  0.99020  3.20   \n",
      "4247      0.145                 26.0                 124.0  0.99274  3.09   \n",
      "\n",
      "      sulphates  alcohol quality  \n",
      "3475       0.56      9.8  medium  \n",
      "1137       0.40     11.7    high  \n",
      "4460       0.43      9.4  medium  \n",
      "4508       0.38     11.5     low  \n",
      "4247       0.56     10.1  medium  \n",
      "Dane po normalizacji: \n",
      "      type  fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
      "1851   1.0       0.190083          0.173333     0.277108        0.062883   \n",
      "5836   0.0       0.280992          0.200000     0.228916        0.033742   \n",
      "3914   1.0       0.198347          0.206667     0.144578        0.064417   \n",
      "713    1.0       0.214876          0.153333     0.234940        0.105828   \n",
      "4384   1.0       0.214876          0.120000     0.126506        0.116564   \n",
      "\n",
      "      chlorides  free sulfur dioxide  total sulfur dioxide   density  \\\n",
      "1851   0.033223             0.069444              0.202765  0.074995   \n",
      "5836   0.098007             0.076389              0.082949  0.124349   \n",
      "3914   0.046512             0.152778              0.304147  0.089262   \n",
      "713    0.051495             0.194444              0.476959  0.147291   \n",
      "4384   0.068106             0.173611              0.405530  0.160208   \n",
      "\n",
      "            pH  sulphates   alcohol quality  \n",
      "1851  0.441860   0.224719  0.623188  medium  \n",
      "5836  0.480620   0.280899  0.710145  medium  \n",
      "3914  0.395349   0.117978  0.463768  medium  \n",
      "713   0.465116   0.117978  0.289855     low  \n",
      "4384  0.395349   0.146067  0.217391     low  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def getData(directory):\n",
    "    data = pd.read_csv(directory)\n",
    "    data = shuffle(data)\n",
    "    data['quality'] = data['quality'].replace([3, 4, 5], 'low')\n",
    "    data['quality'] = data['quality'].replace([6, 7], 'medium')\n",
    "    data['quality'] = data['quality'].replace([8, 9], 'high')\n",
    "    return data\n",
    "\n",
    "def withDeletion(df):\n",
    "    label_encoder = LabelEncoder()\n",
    "    df[\"type\"] = label_encoder.fit_transform(df[\"type\"])\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "def withMean(df):\n",
    "    label_encoder = LabelEncoder()\n",
    "    df[\"type\"] = label_encoder.fit_transform(df[\"type\"])\n",
    "    df = df.fillna(df.mean(numeric_only=True))\n",
    "    return df\n",
    "\n",
    "def normalization(df):\n",
    "    scaler = MinMaxScaler()\n",
    "    columns_to_normalize = df.columns[:-1]\n",
    "    df[columns_to_normalize] = scaler.fit_transform(df[columns_to_normalize])\n",
    "    return df\n",
    "\n",
    "print(\"Wczytane i pomieszane dane: \")\n",
    "print(getData(\"./Data/winequality.csv\").head())\n",
    "print(\"Dane po normalizacji: \")\n",
    "print(normalization(withMean(getData(\"./Data/winequality.csv\"))).head())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-07T17:46:52.275296700Z",
     "start_time": "2023-05-07T17:46:52.203795400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Są tu dwie funkcje odpowiadające za preprocessing danych. Pierwsza z nich usuwa wszystkie wiersze, które zawierają jakąś wartość NaN. Druga z nich zastępuje wartości NaN średnią."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pierwszym z modeli jest Drzewo Decyzyjne, ma ono 2 wersje, 1 gdzie maksymalny poziom wynosi 9, a druga gdzie maksymalny poziom drzewa wynosi 5. Kod wygląda następująco:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def classify(df, train_size):\n",
    "    train_set, test_set = train_test_split(df, train_size=train_size)\n",
    "    classifier = DecisionTreeClassifier(criterion=\"entropy\", max_depth=9, min_samples_leaf=2, splitter=\"best\",\n",
    "                                        class_weight=None, random_state=101)\n",
    "    classifier.fit(train_set.iloc[:, :-1], train_set.iloc[:, -1])\n",
    "    accurarcy = classifier.score(test_set.iloc[:, :-1], test_set.iloc[:, -1])\n",
    "    predicted = classifier.predict(test_set.iloc[:, :-1])\n",
    "    confusionMatrix = confusion_matrix(test_set.iloc[:, -1], predicted)\n",
    "    return accurarcy, confusionMatrix\n",
    "\n",
    "\n",
    "def classifyMax(df, train_size, max_depth):\n",
    "    train_set, test_set = train_test_split(df, train_size=train_size, random_state=2137)\n",
    "    classifier = DecisionTreeClassifier(max_depth=max_depth)\n",
    "    classifier.fit(train_set.iloc[:, :-1], train_set.iloc[:, -1])\n",
    "    accurarcy = classifier.score(test_set.iloc[:, :-1], test_set.iloc[:, -1])\n",
    "    predicted = classifier.predict(test_set.iloc[:, :-1])\n",
    "    confusionMatrix = confusion_matrix(test_set.iloc[:, -1], predicted)\n",
    "    return accurarcy, confusionMatrix"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-07T17:46:52.290297Z",
     "start_time": "2023-05-07T17:46:52.276797600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without max-depth: \n",
      "Without normalization:\n",
      "DT w/ Deletion: \n",
      "Accurarcy:  0.6846986089644513\n",
      "DT w/ Mean: \n",
      "Accurarcy:  0.7169230769230769\n",
      "With normalization:\n",
      "DT w/ Deletion: \n",
      "Accurarcy:  0.7418856259659969\n",
      "DT w/ Mean: \n",
      "Accurarcy:  0.7261538461538461\n",
      "With max-depth: \n",
      "Without normalization:\n",
      "DT w/ Deletion: \n",
      "Accurarcy:  0.714064914992272\n",
      "DT w/ Mean: \n",
      "Accurarcy:  0.703076923076923\n",
      "With normalization:\n",
      "DT w/ Deletion: \n",
      "Accurarcy:  0.714064914992272\n",
      "DT w/ Mean: \n",
      "Accurarcy:  0.703076923076923\n"
     ]
    }
   ],
   "source": [
    "from Methods import DecisionTree\n",
    "from Methods import Preprocessing\n",
    "\n",
    "TRAIN_SIZE = 0.9\n",
    "\n",
    "\n",
    "def main():\n",
    "    dataD = Preprocessing.withDeletion(getData(\"./Data/winequality.csv\"))\n",
    "    dataM = Preprocessing.withMean(getData(\"./Data/winequality.csv\"))\n",
    "\n",
    "    getResultsOfDT(dataD, dataM)\n",
    "\n",
    "\n",
    "\n",
    "def printResult(result):\n",
    "    print(\"Accurarcy: \", result[0])\n",
    "    # print(\"Confusion Matrix: \")\n",
    "    # print(result[1])\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def getResultsOfDT(dataD, dataM):\n",
    "    print(\"Without max-depth: \")\n",
    "    print(\"Without normalization:\")\n",
    "    print(\"DT w/ Deletion: \")\n",
    "    print(printResult(DecisionTree.classify(dataD, TRAIN_SIZE)), end='')\n",
    "    print(\"DT w/ Mean: \")\n",
    "    print(printResult(DecisionTree.classify(dataM, TRAIN_SIZE)), end='')\n",
    "    print(\"With normalization:\")\n",
    "    print(\"DT w/ Deletion: \")\n",
    "    print(printResult(DecisionTree.classify(Preprocessing.normalization(dataD), TRAIN_SIZE)), end='')\n",
    "    print(\"DT w/ Mean: \")\n",
    "    print(printResult(DecisionTree.classify(Preprocessing.normalization(dataM), TRAIN_SIZE)), end='')\n",
    "    print(\"With max-depth: \")\n",
    "    print(\"Without normalization:\")\n",
    "    print(\"DT w/ Deletion: \")\n",
    "    print(printResult(DecisionTree.classifyMax(dataD, TRAIN_SIZE, 5)), end='')\n",
    "    print(\"DT w/ Mean: \")\n",
    "    print(printResult(DecisionTree.classifyMax(dataM, TRAIN_SIZE, 5)), end='')\n",
    "    print(\"With normalization:\")\n",
    "    print(\"DT w/ Deletion: \")\n",
    "    print(printResult(DecisionTree.classifyMax(Preprocessing.normalization(dataD), TRAIN_SIZE, 5)), end='')\n",
    "    print(\"DT w/ Mean: \")\n",
    "    print(printResult(DecisionTree.classifyMax(Preprocessing.normalization(dataM), TRAIN_SIZE, 5)), end='')\n",
    "\n",
    "main()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-07T17:46:52.724296800Z",
     "start_time": "2023-05-07T17:46:52.291298400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Wyniki są następujące, dla drzewa w wersji z 9 poziomami wyniki są lepsze niż dla tego z 5 poziomami. Przy 5 poziomach normalizacja nie miała znaczenia, gdzie w przypadku 9 poziomów\n",
    "normalizacja pogorszyła wyniki. Ustalenie 9 poziomów wynikało z przetestowania metod z wieloma roznymi parametrami przy użyciu GridSearchCV. W tym wypadku usunięcie danych było lepszym wyjściem."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Kolejnym modelem jest Naiwny Bayes, dla którego kod wygląda następująco:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from Methods import NaiveBayes_KN as NBK\n",
    "\n",
    "def classifyNB(df, train_size):\n",
    "    train_set, test_set = train_test_split(df, train_size=train_size, random_state=2137)\n",
    "\n",
    "    classifier = GaussianNB()\n",
    "    classifier.fit(train_set.iloc[:, :-1], train_set.iloc[:, -1])\n",
    "    accurarcy = classifier.score(test_set.iloc[:, :-1], test_set.iloc[:, -1])\n",
    "    predicted = classifier.predict(test_set.iloc[:, :-1])\n",
    "    confusionMatrix = confusion_matrix(test_set.iloc[:, -1], predicted)\n",
    "    return accurarcy, confusionMatrix"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-07T17:46:52.743296200Z",
     "start_time": "2023-05-07T17:46:52.727797500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without normalization:\n",
      "NB w/ Deletion: \n",
      "Accurarcy:  0.642967542503864\n",
      "NB w/ Mean: \n",
      "Accurarcy:  0.6092307692307692\n",
      "With normalization:\n",
      "NB w/ Deletion: \n",
      "Accurarcy:  0.6290571870170015\n",
      "NB w/ Mean: \n",
      "Accurarcy:  0.5984615384615385\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    dataD = Preprocessing.withDeletion(getData(\"./Data/winequality.csv\"))\n",
    "    dataM = Preprocessing.withMean(getData(\"./Data/winequality.csv\"))\n",
    "\n",
    "    getResultsOfNB(dataD, dataM)\n",
    "\n",
    "def getResultsOfNB(dataD, dataM):\n",
    "    print(\"Without normalization:\")\n",
    "    print(\"NB w/ Deletion: \")\n",
    "    print(printResult(NBK.classifyNB(dataD, TRAIN_SIZE)), end='')\n",
    "    print(\"NB w/ Mean: \")\n",
    "    print(printResult(NBK.classifyNB(dataM, TRAIN_SIZE)), end='')\n",
    "    print(\"With normalization:\")\n",
    "    print(\"NB w/ Deletion: \")\n",
    "    print(printResult(NBK.classifyNB(Preprocessing.normalization(dataD), TRAIN_SIZE)), end='')\n",
    "    print(\"NB w/ Mean: \")\n",
    "    print(printResult(NBK.classifyNB(Preprocessing.normalization(dataM), TRAIN_SIZE)), end='')\n",
    "\n",
    "main()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-07T17:46:52.879794200Z",
     "start_time": "2023-05-07T17:46:52.742296100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Wyniki dla normalizacji jak i bez niej są bardzo podobne. Minimalnie na korzyść tych bez normalizacji. Lepsze wyniki wychodzą dla danych z uzupełnionymi brakami."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Kolejnym modelem jest kNN, dla którego kod wygląda następująco:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def classifyKN(df, train_size, neighbors):\n",
    "    train_set, test_set = train_test_split(df, train_size=train_size, random_state=2137)\n",
    "\n",
    "    classifier = KNeighborsClassifier(n_neighbors=neighbors)\n",
    "    classifier.fit(train_set.iloc[:, :-1], train_set.iloc[:, -1])\n",
    "    accurarcy = classifier.score(test_set.iloc[:, :-1], test_set.iloc[:, -1])\n",
    "    predicted = classifier.predict(test_set.iloc[:, :-1])\n",
    "    confusionMatrix = confusion_matrix(test_set.iloc[:, -1], predicted)\n",
    "    return accurarcy, confusionMatrix"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-07T17:46:52.898796400Z",
     "start_time": "2023-05-07T17:46:52.880798400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without normalization:\n",
      "KNN w/ Deletion: \n",
      "Accurarcy:  0.6414219474497682\n",
      "KNN w/ Mean: \n",
      "Accurarcy:  0.66\n",
      "KNN w/ Deletion: \n",
      "Accurarcy:  0.6491499227202473\n",
      "KNN w/ Mean: \n",
      "Accurarcy:  0.6507692307692308\n",
      "KNN w/ Deletion: \n",
      "Accurarcy:  0.6537867078825348\n",
      "KNN w/ Mean: \n",
      "Accurarcy:  0.6538461538461539\n",
      "With normalization:\n",
      "KNN w/ Deletion: \n",
      "Accurarcy:  0.7357032457496137\n",
      "KNN w/ Mean: \n",
      "Accurarcy:  0.7461538461538462\n",
      "KNN w/ Deletion: \n",
      "Accurarcy:  0.7372488408037094\n",
      "KNN w/ Mean: \n",
      "Accurarcy:  0.7369230769230769\n",
      "KNN w/ Deletion: \n",
      "Accurarcy:  0.7341576506955177\n",
      "KNN w/ Mean: \n",
      "Accurarcy:  0.7215384615384616\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    dataD = Preprocessing.withDeletion(getData(\"./Data/winequality.csv\"))\n",
    "    dataM = Preprocessing.withMean(getData(\"./Data/winequality.csv\"))\n",
    "    getResultsOfKNN(dataD, dataM)\n",
    "\n",
    "def getResultsOfKNN(dataD, dataM):\n",
    "    print(\"Without normalization:\")\n",
    "    for i in range(10, 13):\n",
    "        print(\"KNN w/ Deletion: \")\n",
    "        print(printResult(NBK.classifyKN(dataD, TRAIN_SIZE, i)), end='')\n",
    "        print(\"KNN w/ Mean: \")\n",
    "        print(printResult(NBK.classifyKN(dataM, TRAIN_SIZE, i)), end='')\n",
    "    print(\"With normalization:\")\n",
    "    for i in range(10, 13):\n",
    "        print(\"KNN w/ Deletion: \")\n",
    "        print(printResult(NBK.classifyKN(Preprocessing.normalization(dataD), TRAIN_SIZE, i)), end='')\n",
    "        print(\"KNN w/ Mean: \")\n",
    "        print(printResult(NBK.classifyKN(Preprocessing.normalization(dataM), TRAIN_SIZE, i)), end='')\n",
    "\n",
    "main()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-07T17:46:54.243795300Z",
     "start_time": "2023-05-07T17:46:52.896798400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Najlepsze wyniki są dla n=12, określiłem to przy użyciu tej pętli tutaj oraz również przy użyciu GridSearchCV. Przy użyciu normalizacji wyniki są zdecydowanie lepsze z nią. Wyniki są również\n",
    "lepsze dla uzupełnienia braków średnią niż usunięciem danych."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ostatnim modelem jest sieć neuronowa, dla której kod wygląda następująco:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "def classify(df, train_size):\n",
    "    train_set, test_set = train_test_split(df, train_size=train_size, random_state=2137)\n",
    "\n",
    "    # param_grid = {\n",
    "    #     'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50), (100, 50, 25), (10, 20, 30, 40), (100,50,100)],\n",
    "    #     'activation': ['relu', 'tanh'],\n",
    "    #     'solver': ['adam', 'sgd'],\n",
    "    # }\n",
    "    #\n",
    "    # classifier = MLPClassifier()\n",
    "    # grid_search = GridSearchCV(classifier, param_grid, cv=5)\n",
    "    # grid_search.fit(train_set.iloc[:, :-1], train_set.iloc[:, -1])\n",
    "    # print(grid_search.best_params_)\n",
    "    # print(grid_search.best_score_)\n",
    "\n",
    "    classifier = MLPClassifier(hidden_layer_sizes=(100, 50, 25), activation='tanh', solver='adam', max_iter=2000)\n",
    "    classifier.fit(train_set.iloc[:, :-1], train_set.iloc[:, -1])\n",
    "\n",
    "    predicted = classifier.predict(test_set.iloc[:, :-1])\n",
    "    confusionMatrix = confusion_matrix(test_set.iloc[:, -1], predicted)\n",
    "    accurarcy = accuracy_score(test_set.iloc[:, -1], predicted)\n",
    "    return accurarcy, confusionMatrix"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-07T17:46:54.272294800Z",
     "start_time": "2023-05-07T17:46:54.244296500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Użyty znowu został GridSearchCV, aby dobrać najlepsze parametry. Wyniki dla sieci neuronowej są bardzo dobre, a najlepsze są dla 100, 50, 25 neuronów w kolejnych warstwach.\n",
    "Wyniki są następujące:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurarcy:  0.7001545595054096\n",
      "Neural w/ Mean: \n",
      "Accurarcy:  0.7461538461538462\n",
      "With normalization:\n",
      "Neural w/ Deletion: \n",
      "Accurarcy:  0.749613601236476\n",
      "Neural w/ Mean: \n",
      "Accurarcy:  0.7830769230769231\n",
      "Without normalization:\n",
      "Neural w/ Deletion: \n",
      "Accurarcy:  0.7047913446676971\n",
      "Neural w/ Mean: \n",
      "Accurarcy:  0.7261538461538461\n",
      "With normalization:\n",
      "Neural w/ Deletion: \n"
     ]
    }
   ],
   "source": [
    "from Methods import Neural\n",
    "\n",
    "def main():\n",
    "    dataD = Preprocessing.withDeletion(getData(\"./Data/winequality.csv\"))\n",
    "    dataM = Preprocessing.withMean(getData(\"./Data/winequality.csv\"))\n",
    "\n",
    "    getResultsOfNeural(dataD, dataM)\n",
    "\n",
    "\n",
    "LAYERS = (10, 7)\n",
    "\n",
    "\n",
    "def getResultsOfNeural(dataD, dataM):\n",
    "    print(\"Without normalization:\")\n",
    "    print(\"Neural w/ Deletion: \")\n",
    "    print(printResult(Neural.classify(dataD, TRAIN_SIZE, LAYERS)), end='')\n",
    "    print(\"Neural w/ Mean: \")\n",
    "    print(printResult(Neural.classify(dataM, TRAIN_SIZE, LAYERS)), end='')\n",
    "    print(\"With normalization:\")\n",
    "    print(\"Neural w/ Deletion: \")\n",
    "    print(printResult(Neural.classify(Preprocessing.normalization(dataD), TRAIN_SIZE, LAYERS)), end='')\n",
    "    print(\"Neural w/ Mean: \")\n",
    "    print(printResult(Neural.classify(Preprocessing.normalization(dataM), TRAIN_SIZE, LAYERS)), end='')\n",
    "\n",
    "main()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Wyniki są lepsze dla danych z normalizacją oraz dla uzupełnienia danych średnią."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Macierze błedów dla wszystkich modeli wyglądają następująco:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without max-depth: \n",
      "Without normalization:\n",
      "DT w/ Deletion: \n",
      "Confusion Matrix: \n",
      "[[  5   1   6]\n",
      " [  2 159  78]\n",
      " [  8  75 313]]\n",
      "DT w/ Mean: \n",
      "Confusion Matrix: \n",
      "[[  5   1   8]\n",
      " [  0 163  71]\n",
      " [  1  76 325]]\n",
      "With normalization:\n",
      "DT w/ Deletion: \n",
      "Confusion Matrix: \n",
      "[[  9   0  13]\n",
      " [  0 157  78]\n",
      " [  4  77 309]]\n",
      "DT w/ Mean: \n",
      "Confusion Matrix: \n",
      "[[  3   0  15]\n",
      " [  1 166  75]\n",
      " [  5  85 300]]\n",
      "With max-depth: \n",
      "Without normalization:\n",
      "DT w/ Deletion: \n",
      "Confusion Matrix: \n",
      "[[  0   1  18]\n",
      " [  0 136 103]\n",
      " [  0  68 321]]\n",
      "DT w/ Mean: \n",
      "Confusion Matrix: \n",
      "[[  1   0  23]\n",
      " [  0 157 102]\n",
      " [  1  66 300]]\n",
      "With normalization:\n",
      "DT w/ Deletion: \n",
      "Confusion Matrix: \n",
      "[[  0   1  18]\n",
      " [  0 136 103]\n",
      " [  0  68 321]]\n",
      "DT w/ Mean: \n",
      "Confusion Matrix: \n",
      "[[  1   0  23]\n",
      " [  0 157 102]\n",
      " [  1  66 300]]\n",
      "Without normalization:\n",
      "NB w/ Deletion: \n",
      "Confusion Matrix: \n",
      "[[  4   2  13]\n",
      " [  1 138 100]\n",
      " [ 33  89 267]]\n",
      "NB w/ Mean: \n",
      "Confusion Matrix: \n",
      "[[  9   2  13]\n",
      " [  1 145 113]\n",
      " [ 34  86 247]]\n",
      "With normalization:\n",
      "NB w/ Deletion: \n",
      "Confusion Matrix: \n",
      "[[  4   2  13]\n",
      " [  1 138 100]\n",
      " [ 33  89 267]]\n",
      "NB w/ Mean: \n",
      "Confusion Matrix: \n",
      "[[  9   2  13]\n",
      " [  1 145 113]\n",
      " [ 34  86 247]]\n",
      "Without normalization:\n",
      "KNN w/ Deletion: \n",
      "Confusion Matrix: \n",
      "[[  3   0  16]\n",
      " [  0 163  76]\n",
      " [  4  71 314]]\n",
      "KNN w/ Mean: \n",
      "Confusion Matrix: \n",
      "[[  1   0  23]\n",
      " [  0 184  75]\n",
      " [  2  73 292]]\n",
      "KNN w/ Deletion: \n",
      "Confusion Matrix: \n",
      "[[  2   0  17]\n",
      " [  0 152  87]\n",
      " [  4  58 327]]\n",
      "KNN w/ Mean: \n",
      "Confusion Matrix: \n",
      "[[  0   0  24]\n",
      " [  0 170  89]\n",
      " [  2  59 306]]\n",
      "KNN w/ Deletion: \n",
      "Confusion Matrix: \n",
      "[[  2   0  17]\n",
      " [  0 161  78]\n",
      " [  4  67 318]]\n",
      "KNN w/ Mean: \n",
      "Confusion Matrix: \n",
      "[[  0   0  24]\n",
      " [  0 177  82]\n",
      " [  4  68 295]]\n",
      "With normalization:\n",
      "KNN w/ Deletion: \n",
      "Confusion Matrix: \n",
      "[[  3   0  16]\n",
      " [  0 163  76]\n",
      " [  4  71 314]]\n",
      "KNN w/ Mean: \n",
      "Confusion Matrix: \n",
      "[[  1   0  23]\n",
      " [  0 184  75]\n",
      " [  2  73 292]]\n",
      "KNN w/ Deletion: \n",
      "Confusion Matrix: \n",
      "[[  2   0  17]\n",
      " [  0 152  87]\n",
      " [  4  58 327]]\n",
      "KNN w/ Mean: \n",
      "Confusion Matrix: \n",
      "[[  0   0  24]\n",
      " [  0 170  89]\n",
      " [  2  59 306]]\n",
      "KNN w/ Deletion: \n",
      "Confusion Matrix: \n",
      "[[  2   0  17]\n",
      " [  0 161  78]\n",
      " [  4  67 318]]\n",
      "KNN w/ Mean: \n",
      "Confusion Matrix: \n",
      "[[  0   0  24]\n",
      " [  0 177  82]\n",
      " [  4  68 295]]\n",
      "Without normalization:\n",
      "Neural w/ Deletion: \n",
      "Confusion Matrix: \n",
      "[[  6   0  13]\n",
      " [  1 174  64]\n",
      " [  3  66 320]]\n",
      "Neural w/ Mean: \n",
      "Confusion Matrix: \n",
      "[[  4   0  20]\n",
      " [  1 191  67]\n",
      " [  5  84 278]]\n",
      "With normalization:\n",
      "Neural w/ Deletion: \n",
      "Confusion Matrix: \n",
      "[[  6   0  13]\n",
      " [  0 173  66]\n",
      " [  2  70 317]]\n",
      "Neural w/ Mean: \n",
      "Confusion Matrix: \n",
      "[[  1   0  23]\n",
      " [  0 183  76]\n",
      " [  6  67 294]]\n"
     ]
    },
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    dataD = Preprocessing.withDeletion(getData(\"./Data/winequality.csv\"))\n",
    "    dataM = Preprocessing.withMean(getData(\"./Data/winequality.csv\"))\n",
    "\n",
    "    getResultsOfDT(dataD, dataM)\n",
    "    getResultsOfNB(dataD, dataM)\n",
    "    getResultsOfKNN(dataD, dataM)\n",
    "    getResultsOfNeural(dataD, dataM)\n",
    "\n",
    "    return 0\n",
    "\n",
    "\n",
    "def getData(directory):\n",
    "    data = pd.read_csv(directory)\n",
    "    data = shuffle(data)\n",
    "    data['quality'] = data['quality'].replace([3, 4, 5], 'low')\n",
    "    data['quality'] = data['quality'].replace([6, 7], 'medium')\n",
    "    data['quality'] = data['quality'].replace([8, 9], 'high')\n",
    "    return data\n",
    "\n",
    "\n",
    "def printResult(result):\n",
    "    print(\"Confusion Matrix: \")\n",
    "    print(result[1])\n",
    "    return \"\"\n",
    "\n",
    "main()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-07T17:46:52.197294Z",
     "start_time": "2023-05-07T17:43:01.827045600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Podsumowująć w większości przypadków normalizacja danych daje lepsze wyniki, a uzupełnienie braków danych średnią daje lepsze wyniki niż samo usunięcie danych. Najlepszym modelem okazała się sieć neuronowa oraz kNN, gdzie wyniki były na poziomie 75%. Najgorszym modelem okazał się naiwny klasyfikator bayesowski, gdzie wyniki były na poziomie 60%. Wszystkie modele testowane były na 90% danych treningowych oraz 10% danych testowych, bo dla takich wyniki były najlepsze."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
